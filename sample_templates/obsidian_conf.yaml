cache_folder: /storage/llm/cache

embeddings:
  embeddings_path: /storage/llm/embeddings
  embedding_model:
    type: sentence_transformer
    model_name: "intfloat/e5-large-v2"
  splade_config:
    n_batch: 2

 # Support for multi-chunking
  chunk_sizes:
    - 1024

  document_settings:
 #  - doc_path: /home/snexus/projects/knowledge-base
  - doc_path: /storage/llm/docs
    exclude_paths:
      - /home/snexus/projects/knowledge-base/daily_notes
      - /home/snexus/projects/knowledge-base/templates
      - /home/snexus/projects/knowledge-base/other_files
      - /home/snexus/projects/knowledge-base/bookmarks
      - /home/snexus/projects/knowledge-base/excalidraw
    scan_extensions: 
      - md
      - pdf
    additional_parser_settings:
      md: 
        skip_first: True
        merge_sections: False
        remove_images: True
        # find_metadata:
        #   description: "description:"
    passage_prefix: "passage: "


semantic_search:
  search_type: similarity # mmr
  max_k: 15

  replace_output_path:
    - substring_search: /home/snexus/projects/knowledge-base
      substring_replace: obsidian://advanced-uri?vault=knowledge-base&filepath=

  append_suffix:
    append_template: "&heading={heading}"

  max_char_size: 4096
  query_prefix: "query: "

persist_response_db_path: responses_test.db


llm:
   type: openai
   params:
     prompt_template: |
       Contex information is provided below. Given only the context and not prior knowledge, provide detailed answer to the question and references to the provided context. If answer isn't in the context, say you don't know.
        
         ### Context:
         ---------------------
         {context}
         ---------------------

         ### Question: {question}
     model_kwargs:
       temperature: 0.0
       model_name: gpt-3.5-turbo


#llm:
#  type: llamacpp
#  params:
#    model_path: /storage/llm/cache/WizardLM-13B-1.0-GGML/WizardLM-13B-1.0.ggmlv3.q5_K_S.bin
#    prompt_template: |
#          ### Instruction:
#          Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.
#
#          ### Context:
#          ---------------
#          {context}
#          ---------------
#
#          ### Question: {question}
#          ### Response:
#    model_init_params:
#      n_ctx: 1512
#      n_batch: 512
#      n_gpu_layers: 25
#
#    model_kwargs:
#      max_tokens: 512
#      top_p: 0.1
#      top_k: 40
#      temperature: 0.2
#      # mirostat_mode: 1
#



### An attempt to load 33B model on RTX 3060 with 10GB VRAM
# llm:
#   type: llamacpp
#   params:
#     model_path: /storage/llm/cache/airoboros-33b-ggml/airoboros-33b-gpt4-1.2.ggmlv3.q4_K_S.bin
#     prompt_template: |
#           ### Instruction:
#           Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.
#
#           ### Context:
#           ---------------
#           {context}
#           ---------------
#
#           ### Question: {question}
#           ### Response:
#     model_kwargs:
#       n_ctx: 1024
#       max_tokens: 512
#       temperature: 0.0
#       n_gpu_layers: 20
#       n_batch: 512

