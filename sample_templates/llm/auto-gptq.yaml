# This file contains a configuration section relevant to LLM, not the entire config

llm:
  type: auto-gptq
  params:
    model_folder: /storage/llm/cache/tulu-7B-GPTQ
    prompt_template: |
          ### Instruction:
          Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

          ### Context: 
          ---------------
          {context}
          ---------------

          ### Question: {question}
    
    use_safe_tensors: True
    trust_remote_code: False
    device: auto
    model_kwargs:
      quantize_confg: None
      use_triton: False
    pipeline_kwargs:
      temperature: 0
      top_p: 0.2
      max_new_tokens: 1024
      repetition_penalty: 1.15