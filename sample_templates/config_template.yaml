cache_folder:  ## specify a cache folder for embeddings models, huggingface and sentence transformers

embeddings:
  doc_path:  ## specify the docs folder
  embeddings_path: ## specify a folder where embeddings will be saves
  scan_extension: md ## scam extension, currently md is supported

semantic_search:
  search_type: mmr # mmr or neraest_search
  
  # This setting is useful to format paths of the found documents - e.g. form proper links, etc.
  replace_output_path:
    substring_search: ## will replace this string in the path of the document
    substring_replace: ## with this oath
  
  max_char_size: 2048  # Will ensure that context provided to LLM is less than max_char_size

### LLM configuration - check how to configure additional types in sample_templates/llm
llm:
  type: llamacpp # other types - openai, auto-gptq, huggingface
  params: # parameters are type dependent - check out sample_templates/llm
    model_path: /storage/llm/cache/WizardLM-13B-1.0-GGML/WizardLM-13B-1.0.ggmlv3.q5_K_S.bin
    prompt_template: |
          ### Instruction:
          Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

          ### Context: 
          ---------------
          {context}
          ---------------

          ### Question: {question}
          ### Response:
    model_kwargs:
      n_ctx: 1024
      max_tokens: 512
      temperature: 0.0
      n_gpu_layers: 30
      n_batch: 512

# llm:
#   type: openai
#   params:
#     prompt_template: |
#         Context information is provided below. Given the context information and not prior knowledge, answer the question. If answer isn't in the context, say that you don't know, don't try to make up an answer.

#         ### Context: 
#         ---------------------
#         {context}
#         ---------------------

#         ### Question: {question}
#     model_kwargs:
#       temperature: 0.0
#       model_name: gpt-3.5-turbo


