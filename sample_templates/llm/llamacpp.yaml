# This file contains a configuration section relevant to LLM, not the entire config

llm:
  type: llamacpp
  params:
    model_path: /storage/llm/cache/WizardLM-13B-1.0-GGML/WizardLM-13B-1.0.ggmlv3.q5_K_S.bin
    prompt_template: |
          ### Instruction:
          Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

          ### Context: 
          ---------------
          {context}
          ---------------

          ### Question: {question}
          ### Response:
    model_kwargs:
      n_ctx: 1024
      max_tokens: 512
      temperature: 0.0
      n_gpu_layers: 30
      n_batch: 512


## An attempt to load 33B model on RTX 3060 with 10GB VRAM
# llm:
#   type: llamacpp
#   params:
#     model_path: /storage/llm/cache/airoboros-33b-ggml/airoboros-33b-gpt4-1.2.ggmlv3.q4_K_S.bin
#     prompt_template: |
#           ### Instruction:
#           Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

#           ### Context: 
#           ---------------
#           {context}
#           ---------------

#           ### Question: {question}
#           ### Response:
#     model_kwargs:
#       n_ctx: 1024
#       max_tokens: 512
#       temperature: 0.0
#       n_gpu_layers: 20
#       n_batch: 512